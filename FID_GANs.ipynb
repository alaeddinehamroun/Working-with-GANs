{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbMyLrWIqcFTg78XAU/Kut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaeddinehamroun/Working-with-GANs/blob/main/FID_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenges With Evaluating GANS\n",
        "\n",
        "\n",
        "\n",
        "*   **Loss is Uniformative of Performance**: loss tells us little about their performance. Unlike with classifiers, where a low loss on a test set indicates superior performance, a low loss for the generator or discriminator suggests that learning has stopped.\n",
        "*   **No Clear Non-human Metric**:  The is no \"perfect\" discriminator that can differentiate reals from fakes.\n",
        "\n",
        "\n",
        "Fréchet Inception Distance is a one method which aims to solve these issues."
      ],
      "metadata": {
        "id": "w9ZQnqmWqS7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "RQnKwpwLr8XI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G9rY_F51p67-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "MvbI2BBFsMH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  '''\n",
        "  Generator Class\n",
        "  Values:\n",
        "      z_dim: the dimension of the noise vector, a scalar\n",
        "      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
        "            (CelebA is rgb, so 3 is the default)\n",
        "      hidden_dim: the inner dimension, a scalar \n",
        "  '''\n",
        "  def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
        "    super(Generator, self).__init__()\n",
        "    self.z_dim = z_dim\n",
        "    # Build the neural network\n",
        "    self.gen = nn.Sequential(\n",
        "        self.make_gen_block(z_dim, hidden_dim * 8),\n",
        "        self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
        "        self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
        "        self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "        self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "    )\n",
        "  \n",
        "  def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "    '''\n",
        "    Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
        "    a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
        "    Parameters:\n",
        "        input_channels: how many channels the input feature representation has\n",
        "        output_channels: how many channels the output feature representation should have\n",
        "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
        "        stride: the stride of the convolution\n",
        "        final_layer: a boolean, true if it is the final layer and false otherwise (affects activation and batchnorm)\n",
        "    '''\n",
        "    if not final_layer:\n",
        "      return nn.Sequential(\n",
        "          nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "          nn.BatchNorm2d(output_channels),\n",
        "          nn.ReLU(inplace=True)\n",
        "      )\n",
        "    else:\n",
        "      return nn.Sequential(\n",
        "          nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "  def forward(self, noise):\n",
        "    '''\n",
        "    Function for completing a forward pass of the generator: Given a noise tensor, returns generated images.\n",
        "    Parameters:\n",
        "        noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "    '''\n",
        "    x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "    return self.gen(x)"
      ],
      "metadata": {
        "id": "3UYUy6NDsNb4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noise"
      ],
      "metadata": {
        "id": "7zVTOsR0wIc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "  '''\n",
        "  Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "  creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "  Parameters:\n",
        "      n_samples: the number of samples to generate, a scalar\n",
        "      z_dim: the dimension of the noise vector, a scalar\n",
        "      device: the device type\n",
        "  '''\n",
        "  return torch.randn(n_samples, z_dim, device=device)"
      ],
      "metadata": {
        "id": "dDgpLYSswJz9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Pre-trained Model"
      ],
      "metadata": {
        "id": "2CZskeYNw77U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 64\n",
        "image_size = 299\n",
        "device = 'cuda'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset = CelebA(\".\", download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "zl00AdDsw_Pc",
        "outputId": "483e6bd7-66ac-454d-ca43-9c1aa646ad95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c50297335e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCelebA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/celeba.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/celeba.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mdownload_file_from_google_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"img_align_celeba.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_response\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Quota exceeded\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;34mf\"The daily quota of the file {filename} is exceeded and it \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;34mf\"can't be downloaded. This is a limitation of Google Drive \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen.load_state_dict(torch.load(f\"pretrained_celeba.pth\", map_location=torch.device(device))[\"gen\"])\n",
        "gen = gen.eval()"
      ],
      "metadata": {
        "id": "-cqHfx7Gx5H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inception-v3 Network\n",
        "\n",
        "Inception-V3 does a good job detecting features and classifying images."
      ],
      "metadata": {
        "id": "EluDLw5Syh_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import inception_v3\n",
        "inception_model = inception_v3(pretrained=False)\n",
        "inception_model.load_state_dict(torch.load(\"inception_v3_google-1a9a5a14.pth\"))\n",
        "inception_model.to(device)\n",
        "inception_model = inception_model.eval() # Evaluation mode"
      ],
      "metadata": {
        "id": "TQK8g_bvyvql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fréchet Inception Distance\n",
        "\n",
        "FID was proposed as an improvement over Inception Score and still uses the Inception-v3 network as part of its calculation. However, instead of using the classification labels of the Inception-v3 network, it uses the output from the layer right before the labels=the feature layer.  "
      ],
      "metadata": {
        "id": "aIAMLsJtzN8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the final fully-connected (fc) layer with an identity function layer to cut off the classification layer\n",
        "inception_model.fc = nn.Identity()"
      ],
      "metadata": {
        "id": "KTywEUayzQ_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fréchet Distance\n",
        "**Univariate Fréchet Distance**\n",
        "\n",
        "the distance between two normal distributions $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$ and standard deviations $\\sigma_X$ and $\\sigma_Y$, as:\n",
        "\n",
        "$$d(X,Y) = (\\mu_X-\\mu_Y)^2 + (\\sigma_X-\\sigma_Y)^2 $$\n",
        "\n",
        "\n",
        "\n",
        "**Multivariate Fréchet Distance**\n",
        "\n",
        "**Covariance**\n",
        "\n",
        "To find the Fréchet distance between two multivariate normal distributions, you first need to find the covariance instead of the standard deviation. The covariance, which is the multivariate version of variance (the square of standard deviation), is represented using a square matrix where the side length is equal to the number of dimensions. Since the feature vectors you will be using have 2048 values/weights, the covariance matrix will be 2048 x 2048. But for the sake of an example, this is a covariance matrix in a two-dimensional space:\n",
        "\n",
        "$\\Sigma = \\left(\\begin{array}{cc} \n",
        "1 & 0\\\\ \n",
        "0 & 1\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "The value at location $(i, j)$ corresponds to the covariance of vector $i$ with vector $j$. Since the covariance of $i$ with $j$ and $j$ with $i$ are equivalent, the matrix will always be symmetric with respect to the diagonal. The diagonal is the covariance of that element with itself. In this example, there are zeros everywhere except the diagonal. That means that the two dimensions are independent of one another, they are completely unrelated.\n",
        "\n",
        "The following code cell will visualize this matrix."
      ],
      "metadata": {
        "id": "BpbyZcgU47Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import MultivariateNormal\n",
        "import seaborn as sns # This is for visualization\n",
        "mean = torch.Tensor([0, 0]) # Center the mean at the origin\n",
        "covariance = torch.Tensor( # This matrix shows independence - there are only non-zero values on the diagonal\n",
        "    [[1, 0],\n",
        "     [0, 1]]\n",
        ")\n",
        "independent_dist = MultivariateNormal(mean, covariance)\n",
        "samples = independent_dist.sample((10000,))\n",
        "res = sns.jointplot(x=samples[:, 0], y=samples[:, 1], kind=\"kde\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_b0RZOt2R1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, here's an example of a multivariate normal distribution that has covariance:\n",
        "\n",
        "$\\Sigma = \\left(\\begin{array}{cc} \n",
        "2 & -1\\\\ \n",
        "-1 & 2\n",
        "\\end{array}\\right)\n",
        "$\n",
        "\n",
        "And see how it looks:\n"
      ],
      "metadata": {
        "id": "pGttGHxP6Gx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.Tensor([0, 0])\n",
        "covariance = torch.Tensor(\n",
        "    [[2, -1],\n",
        "     [-1, 2]]\n",
        ")\n",
        "covariant_dist = MultivariateNormal(mean, covariance)\n",
        "samples = covariant_dist.sample((10000,))\n",
        "res = sns.jointplot(x = samples[:, 0], y =samples[:, 1], kind=\"kde\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jyILxboc6KOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula**\n",
        "\n",
        "the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:\n",
        "\n",
        "$d(X, Y) = \\Vert\\mu_X-\\mu_Y\\Vert^2 + \\mathrm{Tr}\\left(\\Sigma_X+\\Sigma_Y - 2 \\sqrt{\\Sigma_X \\Sigma_Y}\\right)$\n"
      ],
      "metadata": {
        "id": "QEylolKD6QYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "def matrix_sqrt(x):\n",
        "  '''\n",
        "  Function that takes in a matrix and returns the square root of that matrix.\n",
        "  Parameters:\n",
        "      x: a matrix\n",
        "  '''\n",
        "  y = x.cpu().detach().numpy()\n",
        "  y = scipy.linalg.sqrtm(y)\n",
        "  return torch.Tensor(y.real, device=x.device)"
      ],
      "metadata": {
        "id": "Tjndf5zL6YTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frechet_distance(mu_x, mu_y, sigma_x, sigma_y):\n",
        "  '''\n",
        "  Function for returning the Fréchet distance between multivariate Gaussians,\n",
        "  parameterized by their means and covariance matrices.\n",
        "  Parameters:\n",
        "      mu_x: the mean of the first Gaussian, (n_features)\n",
        "      mu_y: the mean of the second Gaussian, (n_features)\n",
        "      sigma_x: the covariance matrix of the first Gaussian, (n_features, n_features)\n",
        "      sigma_y: the covariance matrix of the second Gaussian, (n_features, n_features)\n",
        "  '''\n",
        "  return (mu_x - mu_y).dot(mu_x - mu_y) + torch.trace(sigma_x) + torch.trace(sigma_y) - 2*torch.trace(matrix_sqrt(sigma_x @ sigma_y))"
      ],
      "metadata": {
        "id": "QbXbguC16x8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all together"
      ],
      "metadata": {
        "id": "-crL9sC_7mNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "  img = torch.nn.functional.interpolate(img, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "  return img"
      ],
      "metadata": {
        "id": "2vOIuQNv7ozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_covariance(features):\n",
        "  return torch.Tensor(np.cov(features.detach().numpy(), rowvar=False))"
      ],
      "metadata": {
        "id": "wSYetRFv7wgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the featurs of the real and fake images using the Inception-v3 model:\n",
        "fake_features_list = []\n",
        "real_features_list = []\n",
        "\n",
        "gen.eval()\n",
        "n_samples = 512 # The total number of samples\n",
        "batch_size = 4 # Samples per iteration\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "cur_samples = 0\n",
        "with torch.no_grad(): # You don't need to calculate gradients here, so you do this to save memory\n",
        "    try:\n",
        "        for real_example, _ in tqdm(dataloader, total=n_samples // batch_size): # Go by batch\n",
        "            real_samples = real_example\n",
        "            real_features = inception_model(real_samples.to(device)).detach().to('cpu') # Move features to CPU\n",
        "            real_features_list.append(real_features)\n",
        "\n",
        "            fake_samples = get_noise(len(real_example), z_dim).to(device)\n",
        "            fake_samples = preprocess(gen(fake_samples))\n",
        "            fake_features = inception_model(fake_samples.to(device)).detach().to('cpu')\n",
        "            fake_features_list.append(fake_features)\n",
        "            cur_samples += len(real_samples)\n",
        "            if cur_samples > n_samples:\n",
        "                break\n",
        "    except:\n",
        "        print(\"Error in loop\")"
      ],
      "metadata": {
        "id": "GrFlBp0671qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all of the values into large tensors\n",
        "fake_features_all = torch.cat(fake_features_list)\n",
        "real_features_all = torch.cat(real_features_list)"
      ],
      "metadata": {
        "id": "-NQqDKzw7_1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Covariance and means of these real and fake features:\n",
        "mu_fake = fake_features_all.mean(0)\n",
        "mu_real = real_features_all.mean(0)\n",
        "sigma_fake = get_covariance(fake_features_all)\n",
        "sigma_real = get_covariance(real_features_all)"
      ],
      "metadata": {
        "id": "LqUV6vaZ8LX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize what the pairwise multivariate distributions of the inception features look like\n",
        "indices = [2, 4, 5]\n",
        "fake_dist = MultivariateNormal(mu_fake[indices], sigma_fake[indices][:, indices])\n",
        "fake_samples = fake_dist.sample((5000,))\n",
        "real_dist = MultivariateNormal(mu_real[indices], sigma_real[indices][:, indices])\n",
        "real_samples = real_dist.sample((5000,))\n",
        "\n",
        "import pandas as pd\n",
        "df_fake = pd.DataFrame(fake_samples.numpy(), columns=indices)\n",
        "df_real = pd.DataFrame(real_samples.numpy(), columns=indices)\n",
        "df_fake[\"is_real\"] = \"no\"\n",
        "df_real[\"is_real\"] = \"yes\"\n",
        "df = pd.concat([df_fake, df_real])\n",
        "sns.pairplot(data = df, plot_kws={'alpha': 0.1}, hue='is_real')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9NRck3Ub8VGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the FID and evaluate you GAN\n",
        "with torch.no_grad():\n",
        "    print(frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake).item())"
      ],
      "metadata": {
        "id": "uWODR8vP8fcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}